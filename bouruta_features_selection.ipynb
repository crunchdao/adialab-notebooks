{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32133101-5601-4d38-815c-379d1e41dc2f",
   "metadata": {},
   "source": [
    "# A prediction using Boruta feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c04d3-46b9-403d-988b-0e7687961a73",
   "metadata": {},
   "source": [
    "The notebook shown here is another version of the basic submission notebook. The method differs in that a feature selection step based on Boruta's method is added before the training step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb407a01-8f72-4b91-8bf0-eb4f0e9ec894",
   "metadata": {},
   "source": [
    "## Set up your crunch workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f46479-07ce-437f-ad06-879efea006d2",
   "metadata": {},
   "source": [
    "#### STEP 1\n",
    "Run this cell to install the crunch library in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca55a51-4466-473f-8cf2-37cdedce9b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install crunch-cli --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61123f18-324c-423b-a55a-be3c98d85558",
   "metadata": {},
   "source": [
    "#### STEP 2\n",
    "Import the crunch package and instantiate it to be able to access its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4c4ec-dd5c-4ce3-9214-6cb8bd3ac081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import crunch\n",
    "crunch = crunch.load_notebook(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe503bd-0201-4acb-8132-4f2d5d670d68",
   "metadata": {},
   "source": [
    "#### STEP 3\n",
    "Go to your submit page and copy paste your setup command to access the data https://adialab.crunchdao.com/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104962d6-b92f-4665-b2a8-2656ff8c0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install crunch-cli --upgrade\n",
    "!crunch --notebook setup resonant-poincare --token qVV38ClZK5806ydJrrxkk1Hb3uRHvocAJBPakm7Vd7mIKUw71qLV9NG6AHImzSJE\n",
    "%cd resonant-poincare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300e44d-9e75-47d1-873a-e767e30136a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import typing\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26b13f-3d77-43b0-bef2-d39693067818",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050d77a-d07c-4bfc-b72d-c44fc8c0d864",
   "metadata": {},
   "source": [
    "Feature selection is the process of identifying and selecting a subset of input variables that are most relevant to predict the target variable. A way to apply feature selection to a dataset is Boruta's method. We choose to use RandomForestRegressor to do the feature selection. The method is described as follows:\n",
    "- First, we create the shadow features randomly shuffing existing ones from Xtrain. In this way we obtain a subset of features whose data generating process is purely white (Gaussian) noise. In this notebook, a batch of dates, identifying cross-sectional representation of the system, are preprocessed together. In fact, in this example, the fitness function of the regressor is not compute on a minibatch, but on individual entries of the dataset. In a context in which the fitness is a function of a minibatch, the shadow features can be generated cross-sectionally.\n",
    "- Then, we apply RandomForestRegressor to the dataset composed of both the features of Xtrain and the shadow features. We get an importance score for each feature and for each shadow feature.\n",
    "- Finally, the features of Xtrain with a lower importance score than the maximum one among the shadow features are removed from the dataset.\n",
    "- We iterate until the number of original features in Xtrain converges.\n",
    "\n",
    "The three following functions are used to set up the above method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3023f-7158-44fc-9053-ce9b8c9d976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def get_X_boruta(X_train):\n",
    "    X_shadow = X_train.apply(np.random.permutation)\n",
    "    X_shadow.columns = ['shadow_' + feat for feat in X_train.columns]\n",
    "    X_boruta = pd.concat([X_train, X_shadow], axis=1)\n",
    "    return X_boruta\n",
    "\n",
    "def get_random_bar(X_boruta, y, X_train, model):\n",
    "    model.fit(X_boruta, np.array(y).ravel())\n",
    "    feat_imp_X = model.feature_importances_[:len(X_train.columns)]\n",
    "    feat_imp_shadow = model.feature_importances_[len(X_train.columns):]\n",
    "    hits = feat_imp_X > feat_imp_shadow.max()\n",
    "    return feat_imp_X, feat_imp_shadow, hits\n",
    "\n",
    "def get_relevant_features(X_train, hits):\n",
    "    features = X_train.columns.values\n",
    "    relevant_features = []\n",
    "    for index, value in enumerate(hits):\n",
    "        if value == True:\n",
    "            relevant_features.append(features[index])\n",
    "    return relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05574282-29b1-4529-b6f8-9bc725091458",
   "metadata": {},
   "source": [
    "The two following functions aim to store the names of the selected features in a json file and load it back during the prediction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24461f-ba83-4d0e-9c4e-89f13085b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_list_as_json(data_list, folder_path, file_name):\n",
    "    json_data = json.dumps(data_list)\n",
    "    file_path = folder_path + \"/\" + file_name\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "        \n",
    "        \n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        json_data = json_file.read()\n",
    "    data_list = json.loads(json_data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741b65d-f14d-48fc-a2ff-8bc815cbaff0",
   "metadata": {},
   "source": [
    "## Train and prediction steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88d6d7-fc12-4af2-a36b-e262cdae20f5",
   "metadata": {},
   "source": [
    "As the dataset contains a huge amount of data, we decided to cut it down to the last 100 dates available in Xtrain. The number of selected dates is represented by the variable n_cut. You are free to change it, but be careful not to exceed the CPU hours assigned to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232cd76-4544-4fef-b111-11d53e671a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train: pd.DataFrame, y_train: pd.DataFrame, model_directory_path: str = \"resources\") -> None:\n",
    "    \"\"\"\n",
    "    Do your model training here.\n",
    "    At each retrain this function will have to save an updated version of\n",
    "    the model under the model_directiory_path, as in the example below.\n",
    "    Note: You can use other serialization methods than joblib.dump(), as\n",
    "    long as it matches what reads the model in infer().\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: the data to train the model.\n",
    "        model_directory_path: the path to save your updated model\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cutting the dataset to the last n_cut dates.\n",
    "    n_cut = 5\n",
    "    \n",
    "    cut_df_X = X_train[X_train['date'] >= int(len(X_train['date'].unique()) - n_cut)]\n",
    "    cut_df_y = y_train[y_train['date'] >= int(len(y_train['date'].unique()) - n_cut)]\n",
    "    \n",
    "    X_train = cut_df_X.drop(columns=['id', 'date'])\n",
    "    y_train = cut_df_y.drop(columns=['id', 'date'])\n",
    "    \n",
    "    \n",
    "    # Feature selection step\n",
    "    model = RandomForestRegressor(n_estimators=30, max_depth=3, n_jobs=30, random_state=42)\n",
    "    \n",
    "    n_iter = 100\n",
    "    n_relevant_features = [len(X_train.columns)]\n",
    "\n",
    "    rel_Xtrain = X_train.copy()\n",
    "\n",
    "    for i in range(1,n_iter+1):\n",
    "        X_boruta = get_X_boruta(rel_Xtrain)\n",
    "        feat_importance_X, feat_importance_shadow, hits = get_random_bar(X_boruta, y_train, rel_Xtrain, model)\n",
    "        relevant_features = get_relevant_features(rel_Xtrain, hits)\n",
    "        n_relevant_features.append(len(relevant_features))\n",
    "        rel_Xtrain = rel_Xtrain.loc[:,relevant_features]\n",
    "        if n_relevant_features[i] == n_relevant_features[i-1]:\n",
    "            break\n",
    "    \n",
    "    relevant_features = rel_Xtrain.columns.to_list()\n",
    "    relevant_features.insert(0, 'date')\n",
    "    relevant_features.insert(0,'id')\n",
    "    \n",
    "    # Saving the selected features in 'resources' file\n",
    "    print(\"saving the selected features\")\n",
    "    folder_path = 'resources'\n",
    "    file_name = 'selected_feat.json'\n",
    "    store_list_as_json(relevant_features, folder_path, file_name)\n",
    "    \n",
    "    # Training the model\n",
    "    print(\"training...\")\n",
    "    model.fit(rel_Xtrain, np.array(y_train).ravel())\n",
    "    print(model)\n",
    "    \n",
    "    # Saving the model in 'resources' file\n",
    "    model_directory_path = 'resources'\n",
    "    model_pathname = Path(model_directory_path) / \"model.joblib\"\n",
    "    print(f\"Saving model in {model_pathname}\")\n",
    "    joblib.dump(model, model_pathname)\n",
    "    \n",
    "\n",
    "    \n",
    "def infer(X_test: pd.DataFrame,\n",
    "          model_directory_path: str = \"resources\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Do your inference here.\n",
    "    This function will load the model saved at the previous iteration and use\n",
    "    it to produce your inference on the current date.\n",
    "    It is mandatory to send your inferences with the ids so the system\n",
    "    can match it correctly.\n",
    "    \n",
    "    Args:\n",
    "        model_directory_path: the path to the directory to the directory in wich we will be saving your updated model.\n",
    "        X_test: the independant  variables of the current date passed to your model.\n",
    "\n",
    "    Returns:\n",
    "        A dataframe (date, id, value) with the inferences of your model for the current date.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loading the selected features\n",
    "    file_path = \"resources/selected_feat.json\"\n",
    "    selected_features = read_json_file(file_path)\n",
    "    \n",
    "    # Loading the model saved by the train function at previous iteration\n",
    "    model = joblib.load(Path(model_directory_path) / \"model.joblib\")\n",
    "    \n",
    "    # Creating the predicted label dataframe with correct dates and ids\n",
    "    X_test = X_test[selected_features]\n",
    "    y_test_predicted = X_test[[\"date\", \"id\"]].copy()\n",
    "    y_test_predicted[\"value\"] = model.predict(X_test.iloc[:, 2:])\n",
    "\n",
    "    return y_test_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33d4a5-ccf2-4385-b85a-3b112bbaac96",
   "metadata": {},
   "source": [
    "## Testing your model locally\n",
    "\n",
    "This last part of the notebook is not necessary for the submission but only for you, to analyze your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca563aec-b523-4128-8e04-6f233be06c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c50d71-d0e6-441a-b498-83922eca3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data\n",
    "X_train, y_train, X_test = crunch.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72111597-1f38-45ae-a258-f6f6f97ed5f0",
   "metadata": {},
   "source": [
    "To have a preliminary assessment of our solution above we need to create train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1bc7a6-9b5e-48d0-a023-c01652fa3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting (X_train, y_train) in X_train_local, X_test_local, y_train_local, y_test_local\")\n",
    "X_train_local, X_test_local, y_train_local, y_test_local = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.2,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc98759-18d8-4c5f-83b0-1f06d7636d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training. It may take a few minutes.\n",
    "train(X_train_local, y_train_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5097167-4a46-4dc3-8f06-894078d4e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inference\")\n",
    "y_test_local_pred = infer(X_test_local, model_directory_path=\"resources\")\n",
    "score = spearmanr(y_test_local[\"y\"], y_test_local_pred[\"value\"])[0] * 100\n",
    "print(f\"Spearman's correlation {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5046f-2f67-4c80-84b0-c9c065123c5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing your submission **locally**\n",
    "\n",
    "This function of the crunch package will run your code locally, in the same way in which the function is called in the cloud (ie: one date at a time). If it runs without problem, it is highly likely that there won't be problems when executing it on the CrunchDAO's system, on the cloud.\n",
    "\n",
    "You can setup the a retraining frequency as you which. A train frequency of 2 means that the system will retrain your model every two dates.\n",
    "\n",
    "`force_first_train=True` means that your model will be trained on the first date of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc05ac5-19c8-4e64-8f4f-db54af556352",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Remove unused data to release memory\")\n",
    "del X_train, y_train, X_test, X_train_local, X_test_local, y_train_local, y_test_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c3781-7555-4bcf-9515-96538a821af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "crunch.test(force_first_train=True, train_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7bd5f-f5d0-4071-84e7-260b210aefb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
